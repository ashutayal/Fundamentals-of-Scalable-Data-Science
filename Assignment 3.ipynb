{"cells": [{"metadata": {}, "cell_type": "markdown", "source": "# Assignment 3\n\nWelcome to Assignment 3. This will be even more fun. Now we will calculate statistical measures. \n\n##\u00a0You only have to pass 4 out of 7 functions\n\nJust make sure you hit the play button on each cell from top to down. There are seven functions you have to implement. Please also make sure than on each change on a function you hit the play button again on the corresponding cell to make it available to the rest of this notebook."}, {"metadata": {}, "cell_type": "markdown", "source": "This notebook is designed to run in a IBM Watson Studio default runtime (NOT the Watson Studio Apache Spark Runtime as the default runtime with 1 vCPU is free of charge). Therefore, we install Apache Spark in local mode for test purposes only. Please don't use it in production.\n\nIn case you are facing issues, please read the following two documents first:\n\nhttps://github.com/IBM/skillsnetwork/wiki/Environment-Setup\n\nhttps://github.com/IBM/skillsnetwork/wiki/FAQ\n\nThen, please feel free to ask:\n\nhttps://coursera.org/learn/machine-learning-big-data-apache-spark/discussions/all\n\nPlease make sure to follow the guidelines before asking a question:\n\nhttps://github.com/IBM/skillsnetwork/wiki/FAQ#im-feeling-lost-and-confused-please-help-me\n\n\nIf running outside Watson Studio, this should work as well. In case you are running in an Apache Spark context outside Watson Studio, please remove the Apache Spark setup in the first notebook cells."}, {"metadata": {}, "cell_type": "code", "source": "from IPython.display import Markdown, display\ndef printmd(string):\n    display(Markdown('# <span style=\"color:red\">'+string+'</span>'))\n\n\nif ('sc' in locals() or 'sc' in globals()):\n    printmd('<<<<<!!!!! It seems that you are running in a IBM Watson Studio Apache Spark Notebook. Please run it in an IBM Watson Studio Default Runtime (without Apache Spark) !!!!!>>>>>')\n", "execution_count": 1, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "!pip install pyspark==2.4.5", "execution_count": 2, "outputs": [{"output_type": "stream", "text": "Collecting pyspark==2.4.5\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9a/5a/271c416c1c2185b6cb0151b29a91fff6fcaed80173c8584ff6d20e46b465/pyspark-2.4.5.tar.gz (217.8MB)\n\u001b[K     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 217.8MB 10.2MB/s eta 0:00:01:00:07     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b        | 160.8MB 10.1MB/s eta 0:00:06\ufffd\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e       | 165.6MB 10.1MB/s eta 0:00:06\n\u001b[?25hCollecting py4j==0.10.7 (from pyspark==2.4.5)\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e3/53/c737818eb9a7dc32a7cd4f1396e787bd94200c3997c72c1dbe028587bd76/py4j-0.10.7-py2.py3-none-any.whl (197kB)\n\u001b[K     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 204kB 47.2MB/s eta 0:00:01\n\u001b[?25hBuilding wheels for collected packages: pyspark\n  Building wheel for pyspark (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Stored in directory: /home/dsxuser/.cache/pip/wheels/bf/db/04/61d66a5939364e756eb1c1be4ec5bdce6e04047fc7929a3c3c\nSuccessfully built pyspark\nInstalling collected packages: py4j, pyspark\nSuccessfully installed py4j-0.10.7 pyspark-2.4.5\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "code", "source": "try:\n    from pyspark import SparkContext, SparkConf\n    from pyspark.sql import SparkSession\nexcept ImportError as e:\n    printmd('<<<<<!!!!! Please restart your kernel after installing Apache Spark !!!!!>>>>>')", "execution_count": 3, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "sc = SparkContext.getOrCreate(SparkConf().setMaster(\"local[*]\"))\n\nspark = SparkSession \\\n    .builder \\\n    .getOrCreate()", "execution_count": 4, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "All functions can be implemented using DataFrames, ApacheSparkSQL or RDDs. We are only interested in the result. You are given the reference to the data frame in the \"df\" parameter and in case you want to use SQL just use the \"spark\" parameter which is a reference to the global SparkSession object. Finally if you want to use RDDs just use \"df.rdd\" for obtaining a reference to the underlying RDD object. But we discurage using RDD at this point in time.\n\nLet's start with the first function. Please calculate the minimal temperature for the test data set you have created. We've provided a little skeleton for you in case you want to use SQL. Everything can be implemented using SQL only if you like."}, {"metadata": {}, "cell_type": "code", "source": "def minTemperature(df):\n    #TODO Please enter your code here, you are not required to use the template code below\n    #some reference: https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame\n    \n    return min(df.temperature)", "execution_count": 8, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Please now do the same for the mean of the temperature"}, {"metadata": {}, "cell_type": "code", "source": "def meanTemperature(df):\n    mean = df['temperature'].mean()\n    #TODO Please enter your code here, you are not required to use the template code below\n    #some reference: https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame\n    return mean", "execution_count": 48, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Please now do the same for the maximum of the temperature"}, {"metadata": {}, "cell_type": "code", "source": "def maxTemperature(df):\n    #TODO Please enter your code here, you are not required to use the template code below\n    #some reference: https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame\n    return max(df.temperature)", "execution_count": 14, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Please now do the same for the standard deviation of the temperature"}, {"metadata": {}, "cell_type": "code", "source": "def sdTemperature(df):\n    mean = df['temperature'].mean()\n    sd = df['temperature'].std()\n    #TODO Please enter your code here, you are not required to use the template code below\n    #some reference: https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame\n    #https://spark.apache.org/docs/2.3.0/api/sql/\n    return sd", "execution_count": 50, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Please now do the same for the skew of the temperature. Since the SQL statement for this is a bit more complicated we've provided a skeleton for you. You have to insert custom code at four positions in order to make the function work. Alternatively you can also remove everything and implement if on your own. Note that we are making use of two previously defined functions, so please make sure they are correct. Also note that we are making use of python's string formatting capabilitis where the results of the two function calls to \"meanTemperature\" and \"sdTemperature\" are inserted at the \"%s\" symbols in the SQL string."}, {"metadata": {}, "cell_type": "code", "source": "def skewTemperature(df):\n    my_list = df[~df['temperature'].isnull()].temperature\n    mean = df['temperature'].mean()\n    sd = df['temperature'].std()\n    kurtosis = sum([(((x - mean)/sd) ** 4) for x in my_list]) / len(my_list) \n    skewness = sum([(((x - mean)/sd) ** 3) for x in my_list]) / len(my_list) \n    return skewness", "execution_count": 61, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Kurtosis is the 4th statistical moment, so if you are smart you can make use of the code for skew which is the 3rd statistical moment. Actually only two things are different."}, {"metadata": {}, "cell_type": "code", "source": "def kurtosisTemperature(df):\n    my_list = df[~df['temperature'].isnull()].temperature\n    mean = df['temperature'].mean()\n    sd = df['temperature'].std()\n    kurtosis = sum([(((x - mean)/sd) ** 4) for x in my_list]) / len(my_list) \n    skewness = sum([(((x - mean)/sd) ** 3) for x in my_list]) / len(my_list) \n    return kurtosis", "execution_count": 62, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Just a hint. This can be solved easily using SQL as well, but as shown in the lecture also using RDDs."}, {"metadata": {}, "cell_type": "code", "source": "import numpy as np", "execution_count": 58, "outputs": [{"output_type": "execute_result", "execution_count": 58, "data": {"text/plain": "0       100.0\n3        86.0\n6        84.0\n7        84.0\n9        96.0\n10       88.0\n13       87.0\n16       96.0\n17       90.0\n18       88.0\n19       80.0\n20       82.0\n21       87.0\n22       96.0\n23       89.0\n25       86.0\n26       84.0\n29       95.0\n30       89.0\n32       90.0\n33       96.0\n34       85.0\n35       84.0\n37       88.0\n39       99.0\n40       96.0\n41       88.0\n44       94.0\n45       99.0\n46       81.0\n        ...  \n2008     87.0\n2009     89.0\n2011     99.0\n2013     99.0\n2015     87.0\n2016     83.0\n2017     83.0\n2018     90.0\n2019     80.0\n2021     82.0\n2023     84.0\n2024     95.0\n2026     82.0\n2028     98.0\n2031     97.0\n2033     89.0\n2035     97.0\n2036     94.0\n2037     90.0\n2040     89.0\n2041    100.0\n2044     86.0\n2045     98.0\n2046     99.0\n2048     90.0\n2049     84.0\n2050     84.0\n2051     99.0\n2052     96.0\n2056     82.0\nName: temperature, Length: 1342, dtype: float64"}, "metadata": {}}]}, {"metadata": {}, "cell_type": "code", "source": "def correlationTemperatureHardness(df):\n    list1 = df[~df['temperature'].isnull()].temperature\n    list2 = df[~df['hardness'].isnull()].hardness\n    joined = list(zip(list1, list2))\n    mean1 = df.temperature.mean()\n    mean2 = df.hardness.mean()\n    sd1 = df.temperature.std()\n    sd2 = df.hardness.std()\n\n    cov = sum([(x-mean1)*(y-mean2) for (x,y) in joined])/len(list1)\n    corr = cov/(sd1*sd2)\n    return corr", "execution_count": 59, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Now it is time to grab a PARQUET file and create a dataframe out of it. Using SparkSQL you can handle it like a database. "}, {"metadata": {}, "cell_type": "code", "source": "!wget https://github.com/IBM/coursera/blob/master/coursera_ds/washing.parquet?raw=true\n!mv washing.parquet?raw=true washing.parquet", "execution_count": 5, "outputs": [{"output_type": "stream", "text": "--2020-08-24 20:03:43--  https://github.com/IBM/coursera/blob/master/coursera_ds/washing.parquet?raw=true\nResolving github.com (github.com)... 140.82.113.4\nConnecting to github.com (github.com)|140.82.113.4|:443... connected.\nHTTP request sent, awaiting response... 301 Moved Permanently\nLocation: https://github.com/IBM/skillsnetwork/blob/master/coursera_ds/washing.parquet?raw=true [following]\n--2020-08-24 20:03:43--  https://github.com/IBM/skillsnetwork/blob/master/coursera_ds/washing.parquet?raw=true\nReusing existing connection to github.com:443.\nHTTP request sent, awaiting response... 302 Found\nLocation: https://github.com/IBM/skillsnetwork/raw/master/coursera_ds/washing.parquet [following]\n--2020-08-24 20:03:43--  https://github.com/IBM/skillsnetwork/raw/master/coursera_ds/washing.parquet\nReusing existing connection to github.com:443.\nHTTP request sent, awaiting response... 302 Found\nLocation: https://raw.githubusercontent.com/IBM/skillsnetwork/master/coursera_ds/washing.parquet [following]\n--2020-08-24 20:03:43--  https://raw.githubusercontent.com/IBM/skillsnetwork/master/coursera_ds/washing.parquet\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.48.133\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.48.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 112048 (109K) [application/octet-stream]\nSaving to: \u2018washing.parquet?raw=true\u2019\n\n100%[======================================>] 112,048     --.-K/s   in 0.007s  \n\n2020-08-24 20:03:44 (15.1 MB/s) - \u2018washing.parquet?raw=true\u2019 saved [112048/112048]\n\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "code", "source": "df = spark.read.parquet('washing.parquet')\ndf.createOrReplaceTempView('washing')\ndf.show()\ndf = df.toPandas()", "execution_count": 11, "outputs": [{"output_type": "stream", "text": "+--------------------+--------------------+-----+--------+----------+---------+--------+-----+-----------+-------------+-------+\n|                 _id|                _rev|count|flowrate|fluidlevel|frequency|hardness|speed|temperature|           ts|voltage|\n+--------------------+--------------------+-----+--------+----------+---------+--------+-----+-----------+-------------+-------+\n|0d86485d0f88d1f9d...|1-57940679fb8a713...|    4|      11|acceptable|     null|      77| null|        100|1547808723923|   null|\n|0d86485d0f88d1f9d...|1-15ff3a0b304d789...|    2|    null|      null|     null|    null| 1046|       null|1547808729917|   null|\n|0d86485d0f88d1f9d...|1-97c2742b68c7b07...|    4|    null|      null|       71|    null| null|       null|1547808731918|    236|\n|0d86485d0f88d1f9d...|1-eefb903dbe45746...|   19|      11|acceptable|     null|      75| null|         86|1547808738999|   null|\n|0d86485d0f88d1f9d...|1-5f68b4c72813c25...|    7|    null|      null|       75|    null| null|       null|1547808740927|    235|\n|0d86485d0f88d1f9d...|1-cd4b6c57ddbe77e...|    5|    null|      null|     null|    null| 1014|       null|1547808744923|   null|\n|0d86485d0f88d1f9d...|1-a35b25b5bf43aaf...|   32|      11|acceptable|     null|      73| null|         84|1547808752028|   null|\n|0d86485d0f88d1f9d...|1-b717f7289a8476d...|   48|      11|acceptable|     null|      79| null|         84|1547808768065|   null|\n|0d86485d0f88d1f9d...|1-c2f1f8fcf178b2f...|   18|    null|      null|       73|    null| null|       null|1547808773944|    228|\n|0d86485d0f88d1f9d...|1-15033dd9eebb4a8...|   59|      11|acceptable|     null|      72| null|         96|1547808779093|   null|\n|0d86485d0f88d1f9d...|1-753dae825f9a6c2...|   62|      11|acceptable|     null|      73| null|         88|1547808782113|   null|\n|0d86485d0f88d1f9d...|1-b168089f44f03f0...|   13|    null|      null|     null|    null| 1097|       null|1547808784940|   null|\n|0d86485d0f88d1f9d...|1-403b687c6be0dea...|   23|    null|      null|       80|    null| null|       null|1547808788955|    236|\n|0d86485d0f88d1f9d...|1-195551e0455a24b...|   72|      11|acceptable|     null|      77| null|         87|1547808792134|   null|\n|0d86485d0f88d1f9d...|1-060a39fc6c2ddee...|   26|    null|      null|       62|    null| null|       null|1547808797959|    233|\n|0d86485d0f88d1f9d...|1-2234514bffee465...|   27|    null|      null|       61|    null| null|       null|1547808800960|    226|\n|0d86485d0f88d1f9d...|1-4265898bb401db0...|   82|      11|acceptable|     null|      79| null|         96|1547808802154|   null|\n|0d86485d0f88d1f9d...|1-2fbf7ca9a0425a0...|   94|      11|acceptable|     null|      73| null|         90|1547808814186|   null|\n|0d86485d0f88d1f9d...|1-203c0ee6d7fbd21...|   97|      11|acceptable|     null|      77| null|         88|1547808817190|   null|\n|0d86485d0f88d1f9d...|1-47e1965db94fcab...|  104|      11|acceptable|     null|      75| null|         80|1547808824198|   null|\n+--------------------+--------------------+-----+--------+----------+---------+--------+-----+-----------+-------------+-------+\nonly showing top 20 rows\n\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "code", "source": "df.head()\nimport numpy as np\ndf = df.fillna()", "execution_count": 47, "outputs": [{"output_type": "execute_result", "execution_count": 47, "data": {"text/plain": "                                _id                                _rev  \\\n0  0d86485d0f88d1f9d60bd193a4306793  1-57940679fb8a713bc46a033b3c495cbe   \n1  0d86485d0f88d1f9d60bd193a4306ee1  1-15ff3a0b304d789e16a71f4b3e540920   \n2  0d86485d0f88d1f9d60bd193a4307322  1-97c2742b68c7b07c2091d817e839f693   \n3  0d86485d0f88d1f9d60bd193a43080fc  1-eefb903dbe4574667b7410be306cf741   \n4  0d86485d0f88d1f9d60bd193a4308701  1-5f68b4c72813c25e12d91004ddb33b1b   \n\n   count  flowrate  fluidlevel  frequency  hardness   speed  temperature  \\\n0      4      11.0  acceptable        NaN      77.0     NaN        100.0   \n1      2       NaN        None        NaN       NaN  1046.0          NaN   \n2      4       NaN        None       71.0       NaN     NaN          NaN   \n3     19      11.0  acceptable        NaN      75.0     NaN         86.0   \n4      7       NaN        None       75.0       NaN     NaN          NaN   \n\n              ts  voltage  \n0  1547808723923      NaN  \n1  1547808729917      NaN  \n2  1547808731918    236.0  \n3  1547808738999      NaN  \n4  1547808740927    235.0  ", "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>_id</th>\n      <th>_rev</th>\n      <th>count</th>\n      <th>flowrate</th>\n      <th>fluidlevel</th>\n      <th>frequency</th>\n      <th>hardness</th>\n      <th>speed</th>\n      <th>temperature</th>\n      <th>ts</th>\n      <th>voltage</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0d86485d0f88d1f9d60bd193a4306793</td>\n      <td>1-57940679fb8a713bc46a033b3c495cbe</td>\n      <td>4</td>\n      <td>11.0</td>\n      <td>acceptable</td>\n      <td>NaN</td>\n      <td>77.0</td>\n      <td>NaN</td>\n      <td>100.0</td>\n      <td>1547808723923</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0d86485d0f88d1f9d60bd193a4306ee1</td>\n      <td>1-15ff3a0b304d789e16a71f4b3e540920</td>\n      <td>2</td>\n      <td>NaN</td>\n      <td>None</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1046.0</td>\n      <td>NaN</td>\n      <td>1547808729917</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0d86485d0f88d1f9d60bd193a4307322</td>\n      <td>1-97c2742b68c7b07c2091d817e839f693</td>\n      <td>4</td>\n      <td>NaN</td>\n      <td>None</td>\n      <td>71.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1547808731918</td>\n      <td>236.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0d86485d0f88d1f9d60bd193a43080fc</td>\n      <td>1-eefb903dbe4574667b7410be306cf741</td>\n      <td>19</td>\n      <td>11.0</td>\n      <td>acceptable</td>\n      <td>NaN</td>\n      <td>75.0</td>\n      <td>NaN</td>\n      <td>86.0</td>\n      <td>1547808738999</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0d86485d0f88d1f9d60bd193a4308701</td>\n      <td>1-5f68b4c72813c25e12d91004ddb33b1b</td>\n      <td>7</td>\n      <td>NaN</td>\n      <td>None</td>\n      <td>75.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1547808740927</td>\n      <td>235.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"}, "metadata": {}}]}, {"metadata": {}, "cell_type": "markdown", "source": "Now let's test the functions you've implemented"}, {"metadata": {}, "cell_type": "code", "source": "min_temperature = 0\nmean_temperature = 0\nmax_temperature = 0\nsd_temperature = 0\nskew_temperature = 0\nkurtosis_temperature = 0\ncorrelation_temperature = 0\n", "execution_count": 36, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "min_temperature = minTemperature(df)\nprint(min_temperature)", "execution_count": 37, "outputs": [{"output_type": "stream", "text": "80.0\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "code", "source": "mean_temperature = meanTemperature(df)\nprint(mean_temperature)", "execution_count": 49, "outputs": [{"output_type": "stream", "text": "90.03800298062593\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "code", "source": "max_temperature = maxTemperature(df)\nprint(max_temperature)", "execution_count": 39, "outputs": [{"output_type": "stream", "text": "100.0\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "code", "source": "sd_temperature = sdTemperature(df)\nprint(sd_temperature)", "execution_count": 51, "outputs": [{"output_type": "stream", "text": "6.100761058621965\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "code", "source": "skew_temperature = skewTemperature(df)\nprint(skew_temperature)", "execution_count": 63, "outputs": [{"output_type": "stream", "text": "0.01039837458333482\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "code", "source": "kurtosis_temperature = kurtosisTemperature(df)\nprint(kurtosis_temperature)", "execution_count": 64, "outputs": [{"output_type": "stream", "text": "1.7734271508770005\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "code", "source": "correlation_temperature = correlationTemperatureHardness(df)\nprint(correlation_temperature)", "execution_count": 65, "outputs": [{"output_type": "stream", "text": "0.017740839487648496\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "markdown", "source": "Congratulations, you are done, please submit this notebook to the grader. \nWe have to install a little library in order to submit to coursera first.\n\nThen, please provide your email address and obtain a submission token on the grader\u2019s submission page in coursera, then execute the subsequent cells\n\n### Note: We've changed the grader in this assignment and will do so for the others soon since it gives less errors\nThis means you can directly submit your solutions from this notebook"}, {"metadata": {}, "cell_type": "code", "source": "!rm -f rklib.py\n!wget https://raw.githubusercontent.com/IBM/coursera/master/rklib.py", "execution_count": 44, "outputs": [{"output_type": "stream", "text": "--2020-08-24 20:15:26--  https://raw.githubusercontent.com/IBM/coursera/master/rklib.py\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.48.133\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.48.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 2540 (2.5K) [text/plain]\nSaving to: \u2018rklib.py\u2019\n\n100%[======================================>] 2,540       --.-K/s   in 0s      \n\n2020-08-24 20:15:27 (30.6 MB/s) - \u2018rklib.py\u2019 saved [2540/2540]\n\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "code", "source": "from rklib import submitAll\nimport json\n\nkey = \"Suy4biHNEeimFQ479R3GjA\"\nemail = 'ashutayal@hotmail.com'\ntoken = 'p5YA3iHTmyAqmVdL'\n\n\n", "execution_count": 45, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "parts_data = {}\nparts_data[\"FWMEL\"] = json.dumps(min_temperature)\nparts_data[\"3n3TK\"] = json.dumps(mean_temperature)\nparts_data[\"KD3By\"] = json.dumps(max_temperature)\nparts_data[\"06Zie\"] = json.dumps(sd_temperature)\nparts_data[\"Qc8bI\"] = json.dumps(skew_temperature)\nparts_data[\"LoqQi\"] = json.dumps(kurtosis_temperature)\nparts_data[\"ehNGV\"] = json.dumps(correlation_temperature)\n\n\n\nsubmitAll(email, token, key, parts_data)", "execution_count": 66, "outputs": [{"output_type": "stream", "text": "Submission successful, please check on the coursera grader page for the status\n-------------------------\n{\"elements\":[{\"itemId\":\"TzU1P\",\"id\":\"sUpST4RAEeawAApvKZgcCQ~TzU1P~v2j4wuZJEeqGfg6P9v-hwQ\",\"courseId\":\"sUpST4RAEeawAApvKZgcCQ\"}],\"paging\":{},\"linked\":{}}\n-------------------------\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "code", "source": "", "execution_count": null, "outputs": []}], "metadata": {"kernelspec": {"name": "python3", "display_name": "Python 3.6", "language": "python"}, "language_info": {"name": "python", "version": "3.6.9", "mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 3}, "pygments_lexer": "ipython3", "nbconvert_exporter": "python", "file_extension": ".py"}}, "nbformat": 4, "nbformat_minor": 1}